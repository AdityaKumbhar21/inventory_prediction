{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c3197c",
   "metadata": {},
   "source": [
    "# Inventory Prediction - Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Advanced ML libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c4cff5",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7270a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "data_path = '../data/processed/processed_sales_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744844fd",
   "metadata": {},
   "source": [
    "## 2. Feature Selection & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15616517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to use for modeling\n",
    "# Exclude non-predictive columns\n",
    "exclude_cols = ['date', 'product', 'store', 'sales', 'year_month', 'inventory_level']\n",
    "\n",
    "# Select feature columns\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Total features selected: {len(feature_cols)}\")\n",
    "print(\"\\nFeature categories:\")\n",
    "print(f\"  Time features: {[c for c in feature_cols if any(t in c for t in ['year', 'month', 'day', 'week', 'quarter', 'weekend'])]}\")\n",
    "print(f\"\\n  Lag features: {[c for c in feature_cols if 'lag' in c]}\")\n",
    "print(f\"\\n  Rolling features: {[c for c in feature_cols if 'rolling' in c]}\")\n",
    "print(f\"\\n  Other features: {[c for c in feature_cols if not any(t in c for t in ['lag', 'rolling', 'year', 'month', 'day', 'week', 'quarter', 'weekend'])]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b30f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X (features) and y (target)\n",
    "X = df[feature_cols].copy()\n",
    "y = df['sales'].copy()\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"  Mean: {y.mean():.2f}\")\n",
    "print(f\"  Std: {y.std():.2f}\")\n",
    "print(f\"  Min: {y.min():.2f}\")\n",
    "print(f\"  Max: {y.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7ae9f",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split (Time-Series Aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a2843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split: Use last 20% of data for testing\n",
    "split_date = df['date'].quantile(0.8)\n",
    "print(f\"Split date: {split_date}\")\n",
    "\n",
    "train_mask = df['date'] < split_date\n",
    "test_mask = df['date'] >= split_date\n",
    "\n",
    "X_train = X[train_mask]\n",
    "X_test = X[test_mask]\n",
    "y_train = y[train_mask]\n",
    "y_test = y[test_mask]\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a46112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled successfully!\")\n",
    "print(f\"Mean of scaled training features: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Std of scaled training features: {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c750051",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "### 4.1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c12446",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Linear Regression...\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = lr_model.predict(X_train_scaled)\n",
    "y_test_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "train_rmse_lr = np.sqrt(mean_squared_error(y_train, y_train_pred_lr))\n",
    "test_rmse_lr = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))\n",
    "train_mae_lr = mean_absolute_error(y_train, y_train_pred_lr)\n",
    "test_mae_lr = mean_absolute_error(y_test, y_test_pred_lr)\n",
    "train_r2_lr = r2_score(y_train, y_train_pred_lr)\n",
    "test_r2_lr = r2_score(y_test, y_test_pred_lr)\n",
    "\n",
    "print(f\"✓ Linear Regression trained!\")\n",
    "print(f\"  Train RMSE: {train_rmse_lr:.2f} | Test RMSE: {test_rmse_lr:.2f}\")\n",
    "print(f\"  Train MAE: {train_mae_lr:.2f} | Test MAE: {test_mae_lr:.2f}\")\n",
    "print(f\"  Train R²: {train_r2_lr:.4f} | Test R²: {test_r2_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0443ce7",
   "metadata": {},
   "source": [
    "### 4.2 Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e50f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Ridge Regression...\")\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_ridge = ridge_model.predict(X_train_scaled)\n",
    "y_test_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "train_rmse_ridge = np.sqrt(mean_squared_error(y_train, y_train_pred_ridge))\n",
    "test_rmse_ridge = np.sqrt(mean_squared_error(y_test, y_test_pred_ridge))\n",
    "train_mae_ridge = mean_absolute_error(y_train, y_train_pred_ridge)\n",
    "test_mae_ridge = mean_absolute_error(y_test, y_test_pred_ridge)\n",
    "train_r2_ridge = r2_score(y_train, y_train_pred_ridge)\n",
    "test_r2_ridge = r2_score(y_test, y_test_pred_ridge)\n",
    "\n",
    "print(f\"✓ Ridge Regression trained!\")\n",
    "print(f\"  Train RMSE: {train_rmse_ridge:.2f} | Test RMSE: {test_rmse_ridge:.2f}\")\n",
    "print(f\"  Train MAE: {train_mae_ridge:.2f} | Test MAE: {test_mae_ridge:.2f}\")\n",
    "print(f\"  Train R²: {train_r2_ridge:.4f} | Test R²: {test_r2_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96eed9e",
   "metadata": {},
   "source": [
    "### 4.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_rmse_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))\n",
    "test_rmse_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))\n",
    "train_mae_rf = mean_absolute_error(y_train, y_train_pred_rf)\n",
    "test_mae_rf = mean_absolute_error(y_test, y_test_pred_rf)\n",
    "train_r2_rf = r2_score(y_train, y_train_pred_rf)\n",
    "test_r2_rf = r2_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(f\"✓ Random Forest trained!\")\n",
    "print(f\"  Train RMSE: {train_rmse_rf:.2f} | Test RMSE: {test_rmse_rf:.2f}\")\n",
    "print(f\"  Train MAE: {train_mae_rf:.2f} | Test MAE: {test_mae_rf:.2f}\")\n",
    "print(f\"  Train R²: {train_r2_rf:.4f} | Test R²: {test_r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4a721",
   "metadata": {},
   "source": [
    "### 4.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2646624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_xgb = xgb_model.predict(X_train)\n",
    "y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_rmse_xgb = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))\n",
    "test_rmse_xgb = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))\n",
    "train_mae_xgb = mean_absolute_error(y_train, y_train_pred_xgb)\n",
    "test_mae_xgb = mean_absolute_error(y_test, y_test_pred_xgb)\n",
    "train_r2_xgb = r2_score(y_train, y_train_pred_xgb)\n",
    "test_r2_xgb = r2_score(y_test, y_test_pred_xgb)\n",
    "\n",
    "print(f\"✓ XGBoost trained!\")\n",
    "print(f\"  Train RMSE: {train_rmse_xgb:.2f} | Test RMSE: {test_rmse_xgb:.2f}\")\n",
    "print(f\"  Train MAE: {train_mae_xgb:.2f} | Test MAE: {test_mae_xgb:.2f}\")\n",
    "print(f\"  Train R²: {train_r2_xgb:.4f} | Test R²: {test_r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6898237",
   "metadata": {},
   "source": [
    "### 4.5 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0876f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training LightGBM...\")\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lgb = lgb_model.predict(X_train)\n",
    "y_test_pred_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_rmse_lgb = np.sqrt(mean_squared_error(y_train, y_train_pred_lgb))\n",
    "test_rmse_lgb = np.sqrt(mean_squared_error(y_test, y_test_pred_lgb))\n",
    "train_mae_lgb = mean_absolute_error(y_train, y_train_pred_lgb)\n",
    "test_mae_lgb = mean_absolute_error(y_test, y_test_pred_lgb)\n",
    "train_r2_lgb = r2_score(y_train, y_train_pred_lgb)\n",
    "test_r2_lgb = r2_score(y_test, y_test_pred_lgb)\n",
    "\n",
    "print(f\"✓ LightGBM trained!\")\n",
    "print(f\"  Train RMSE: {train_rmse_lgb:.2f} | Test RMSE: {test_rmse_lgb:.2f}\")\n",
    "print(f\"  Train MAE: {train_mae_lgb:.2f} | Test MAE: {test_mae_lgb:.2f}\")\n",
    "print(f\"  Train R²: {train_r2_lgb:.4f} | Test R²: {test_r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f2a407",
   "metadata": {},
   "source": [
    "### 4.6 Neural Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac403007",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Neural Network (MLP)...\")\n",
    "mlp_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 50, 25),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_mlp = mlp_model.predict(X_train_scaled)\n",
    "y_test_pred_mlp = mlp_model.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "train_rmse_mlp = np.sqrt(mean_squared_error(y_train, y_train_pred_mlp))\n",
    "test_rmse_mlp = np.sqrt(mean_squared_error(y_test, y_test_pred_mlp))\n",
    "train_mae_mlp = mean_absolute_error(y_train, y_train_pred_mlp)\n",
    "test_mae_mlp = mean_absolute_error(y_test, y_test_pred_mlp)\n",
    "train_r2_mlp = r2_score(y_train, y_train_pred_mlp)\n",
    "test_r2_mlp = r2_score(y_test, y_test_pred_mlp)\n",
    "\n",
    "print(f\"✓ Neural Network trained!\")\n",
    "print(f\"  Train RMSE: {train_rmse_mlp:.2f} | Test RMSE: {test_rmse_mlp:.2f}\")\n",
    "print(f\"  Train MAE: {train_mae_mlp:.2f} | Test MAE: {test_mae_mlp:.2f}\")\n",
    "print(f\"  Train R²: {train_r2_mlp:.4f} | Test R²: {test_r2_mlp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbee63e",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fe181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Ridge', 'Random Forest', 'XGBoost', 'LightGBM', 'Neural Network'],\n",
    "    'Train_RMSE': [train_rmse_lr, train_rmse_ridge, train_rmse_rf, train_rmse_xgb, train_rmse_lgb, train_rmse_mlp],\n",
    "    'Test_RMSE': [test_rmse_lr, test_rmse_ridge, test_rmse_rf, test_rmse_xgb, test_rmse_lgb, test_rmse_mlp],\n",
    "    'Train_MAE': [train_mae_lr, train_mae_ridge, train_mae_rf, train_mae_xgb, train_mae_lgb, train_mae_mlp],\n",
    "    'Test_MAE': [test_mae_lr, test_mae_ridge, test_mae_rf, test_mae_xgb, test_mae_lgb, test_mae_mlp],\n",
    "    'Train_R2': [train_r2_lr, train_r2_ridge, train_r2_rf, train_r2_xgb, train_r2_lgb, train_r2_mlp],\n",
    "    'Test_R2': [test_r2_lr, test_r2_ridge, test_r2_rf, test_r2_xgb, test_r2_lgb, test_r2_mlp]\n",
    "})\n",
    "\n",
    "# Calculate overfitting indicator\n",
    "results['RMSE_Diff'] = results['Train_RMSE'] - results['Test_RMSE']\n",
    "results['R2_Diff'] = results['Train_R2'] - results['Test_R2']\n",
    "\n",
    "# Sort by Test RMSE\n",
    "results = results.sort_values('Test_RMSE')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\" * 100)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = results.iloc[0]['Model']\n",
    "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "print(f\"   Test RMSE: {results.iloc[0]['Test_RMSE']:.2f}\")\n",
    "print(f\"   Test MAE: {results.iloc[0]['Test_MAE']:.2f}\")\n",
    "print(f\"   Test R²: {results.iloc[0]['Test_R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdbff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# RMSE comparison\n",
    "x = np.arange(len(results))\n",
    "width = 0.35\n",
    "axes[0, 0].bar(x - width/2, results['Train_RMSE'], width, label='Train', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, results['Test_RMSE'], width, label='Test', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Model')\n",
    "axes[0, 0].set_ylabel('RMSE')\n",
    "axes[0, 0].set_title('RMSE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(results['Model'], rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE comparison\n",
    "axes[0, 1].bar(x - width/2, results['Train_MAE'], width, label='Train', alpha=0.8)\n",
    "axes[0, 1].bar(x + width/2, results['Test_MAE'], width, label='Test', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Model')\n",
    "axes[0, 1].set_ylabel('MAE')\n",
    "axes[0, 1].set_title('MAE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(results['Model'], rotation=45, ha='right')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# R² comparison\n",
    "axes[1, 0].bar(x - width/2, results['Train_R2'], width, label='Train', alpha=0.8)\n",
    "axes[1, 0].bar(x + width/2, results['Test_R2'], width, label='Test', alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Model')\n",
    "axes[1, 0].set_ylabel('R² Score')\n",
    "axes[1, 0].set_title('R² Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(results['Model'], rotation=45, ha='right')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test RMSE ranking\n",
    "axes[1, 1].barh(results['Model'], results['Test_RMSE'], color='steelblue', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Test RMSE')\n",
    "axes[1, 1].set_title('Test RMSE Ranking (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].invert_yaxis()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372737d",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd849a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from tree-based models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Random Forest\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "axes[0].barh(rf_importance['feature'], rf_importance['importance'])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Random Forest - Top 15 Features', fontsize=12, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# XGBoost\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "axes[1].barh(xgb_importance['feature'], xgb_importance['importance'])\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('XGBoost - Top 15 Features', fontsize=12, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# LightGBM\n",
    "lgb_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': lgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "axes[2].barh(lgb_importance['feature'], lgb_importance['importance'])\n",
    "axes[2].set_xlabel('Importance')\n",
    "axes[2].set_title('LightGBM - Top 15 Features', fontsize=12, fontweight='bold')\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881a58a4",
   "metadata": {},
   "source": [
    "## 7. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual for best model (assuming XGBoost)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# XGBoost - Scatter plot\n",
    "axes[0, 0].scatter(y_test, y_test_pred_xgb, alpha=0.5)\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Sales')\n",
    "axes[0, 0].set_ylabel('Predicted Sales')\n",
    "axes[0, 0].set_title('XGBoost: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# LightGBM - Scatter plot\n",
    "axes[0, 1].scatter(y_test, y_test_pred_lgb, alpha=0.5, color='orange')\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 1].set_xlabel('Actual Sales')\n",
    "axes[0, 1].set_ylabel('Predicted Sales')\n",
    "axes[0, 1].set_title('LightGBM: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals - XGBoost\n",
    "residuals_xgb = y_test - y_test_pred_xgb\n",
    "axes[1, 0].scatter(y_test_pred_xgb, residuals_xgb, alpha=0.5)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1, 0].set_xlabel('Predicted Sales')\n",
    "axes[1, 0].set_ylabel('Residuals')\n",
    "axes[1, 0].set_title('XGBoost: Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals distribution\n",
    "axes[1, 1].hist(residuals_xgb, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Residuals')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('XGBoost: Residual Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4c12b",
   "metadata": {},
   "source": [
    "## 8. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bfb914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained models\n",
    "models = {\n",
    "    'linear_regression': lr_model,\n",
    "    'ridge': ridge_model,\n",
    "    'random_forest': rf_model,\n",
    "    'xgboost': xgb_model,\n",
    "    'lightgbm': lgb_model,\n",
    "    'neural_network': mlp_model\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model_path = f'../models/{name}_model.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"✓ Saved: {model_path}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = '../models/scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"✓ Saved: {scaler_path}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_path = '../models/feature_names.pkl'\n",
    "joblib.dump(feature_cols, feature_path)\n",
    "print(f\"✓ Saved: {feature_path}\")\n",
    "\n",
    "# Save results\n",
    "results_path = '../reports/model_comparison.csv'\n",
    "results.to_csv(results_path, index=False)\n",
    "print(f\"✓ Saved: {results_path}\")\n",
    "\n",
    "print(\"\\nAll models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4437fa",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "1. Loaded preprocessed data\n",
    "2. Split data with time-series awareness\n",
    "3. Trained 6 different ML models:\n",
    "   - Linear Regression\n",
    "   - Ridge Regression\n",
    "   - Random Forest\n",
    "   - XGBoost\n",
    "   - LightGBM\n",
    "   - Neural Network (MLP)\n",
    "4. Compared model performance\n",
    "5. Analyzed feature importance\n",
    "6. Saved all models for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b0b49",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
